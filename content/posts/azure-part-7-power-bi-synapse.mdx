---
title: "Building a Production-Ready Data Pipeline with Azure (Part 7): Power BI Integration with Synapse"
date: 2025-08-09
index: 10
summary: "Surface Azure Synapse and Databricks datasets in Power BI with DirectQuery patterns, caching tactics, and cost controls."
tags:
  - Power BI
  - Synapse
  - DirectQuery
  - Azure
draft: false
---
## Building a Production Ready Data Pipeline with Azure Part 7: Power BI Integration with Synapse Serverless SQL Pool

![](https://miro.medium.com/v2/resize:fit:1400/1*Oxo-u9GMK3zbEM5h-FiVnQ.png)

*Completing our enterprise analytics architecture with cost-effective Power BI integration*

Welcome back to our comprehensive series on building production-ready data pipelines with Azure. Over the past six articles, I’ve meticulously constructed each layer of a modern data platform. Today, I’m bringing it all together with the final piece: integrating Power BI through Synapse Serverless SQL Pool to deliver insights to business users.

## The Journey So Far

Before diving into today’s topic, let me quickly recap what I’ve built:

[**Part 1: Complete Guide to Medallion Architecture](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-complete-guide-to-medallion-architecture-09d73f0e62dd)**
I established the foundation with Azure Data Factory, ADLS Gen2, and implemented the medallion architecture pattern - Bronze, Silver, and Gold layers.

[**Part 2: Unity Catalog Integration](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-2-unity-catalog-integration-689b52cb71b2)**
I enhanced the platform with Unity Catalog for unified governance, implementing fine-grained access controls and data lineage tracking.

[**Part 3: Advanced Unity Catalog Table Management](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-3-advanced-unity-catalog-table-d456edbd7f47)**
I dove deep into external tables, managed tables, and implementing data quality constraints with Unity Catalog.

[**Part 4: From Mount Points to Unity Catalog](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-4-from-mount-points-to-unity-catalog-696f8dd634a1)**
I modernized data access patterns, migrating from traditional mount points to Unity Catalog’s volume management.

[**Part 5: Implementing CI/CD](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-5-implementing-ci-cd-for-azure-data-fb19c679311b)**
I automated the deployment pipeline with Azure DevOps, implementing infrastructure as code and automated testing.

[**Part 6: Implementing the Gold Layer](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-6-implementing-the-gold-layer-with-c10b141dd7f3)**
I created business-ready datasets in the Gold layer, implementing aggregations and business logic with Delta Lake.

Now, let’s complete the architecture by connecting Power BI to the Gold layer in the most cost-effective and performant way.

## Today’s Challenge: Power BI Integration at Scale

After building a Gold layer with carefully crafted star schemas and aggregated datasets, I faced a critical decision: How do I expose this data to Power BI in a way that balances cost, performance, and maintainability?

The stakes were high. Choose wrong, and you could end up with:

- **Thousands of dollars in unnecessary compute costs**
- Slow dashboard refresh times that frustrate business users
- Complex maintenance overhead
- Limited scalability as data grows

## Architecture Overview: The Integration Layer

My architecture connects Power BI to the Gold layer through Synapse Serverless SQL Pool, creating a cost-effective serving layer:

![](https://miro.medium.com/v2/resize:fit:1400/1*Uf7wtPk_nYwbK2IaL2NktA.png)

But why this architecture? Let me show you the analysis that led me here.

## The Critical Decision: Serverless vs Dedicated vs Databricks SQL

When evaluating compute options for Power BI integration, I analyzed three primary options based on specific requirements: 4 daily refreshes, 100GB data scans per refresh, 50 concurrent users during business hours.

## Synapse Serverless SQL Pool

Serverless SQL Pool operates on a pay-per-query model where you only pay for the data scanned. This aligns perfectly with Power BI’s scheduled refresh pattern.

**Technical Specifications:**

- Pricing: $5 per TB of data processed
- Concurrent executing requests: 15 (additional requests are queued)
- Maximum queued requests: 1000
- Query timeout: 30 minutes (cannot be changed)
- No minimum cost or idle charges
- Automatic scaling with no configuration
- Direct query on Delta, Parquet, CSV formats

**Real Cost Calculation:** For my scenario of 4 daily refreshes scanning 100GB each:

- Daily data processed: 400GB
- Daily cost: 0.4 TB × $5 = $2
- Monthly cost: $2 × 30 = $60
- Annual cost: $720

## Synapse Dedicated SQL Pool

Dedicated SQL Pool provides reserved compute capacity with predictable performance characteristics.

**Technical Specifications (West US 3 region):**

- DW100c: $1.20/hour, 4 concurrent queries
- DW200c: $2.40/hour, 8 concurrent queries
- DW300c: $3.60/hour, 12 concurrent queries
- DW400c: $4.80/hour, 16 concurrent queries
- DW500c: $6.00/hour, 20 concurrent queries
- DW1000c: $12.00/hour, 32 concurrent queries
- DW1500c: $18.00/hour, 32 concurrent queries
- DW2000c: $24.00/hour, 48 concurrent queries
- DW3000c: $36.00/hour, 64 concurrent queries
- DW6000c: $72.00/hour, 128 concurrent queries
- Additional storage: $23/TB/month
- Geo-Redundant Disaster Recovery: $0.06/GB/month

**Reservation Options:**

1-year reserved: ~37% savings

3-year reserved: ~65% savings

**Real Cost Calculation:** For DW1000c (minimum recommended for production) in West US 3:

- Hourly cost: $12.00
- Daily cost (24 hours): $288.00
- Monthly cost: $8,640
- Annual cost: $103,680

With 3-year reservation (65% savings):

- Annual cost: ~$36,288
- Monthly cost: ~$3,024

Ref : [Azure Pricing Calculator](https://azure.microsoft.com/en-us/pricing/calculator/?ef_id=_k_CjwKCAjw49vEBhAVEiwADnMbbIWS5ZQBGITtYkRTPYHbHCgNuEkDDPjZVN5UmcfiaIscOBJl_ZdIHRoCN7QQAvD_BwE_k_&OCID=AIDcmm5edswduu_SEM__k_CjwKCAjw49vEBhAVEiwADnMbbIWS5ZQBGITtYkRTPYHbHCgNuEkDDPjZVN5UmcfiaIscOBJl_ZdIHRoCN7QQAvD_BwE_k_&gad_source=1&gad_campaignid=21496728177&gbraid=0AAAAADcJh_tIM3QN2YaSaEEG8mw70IIDn&gclid=CjwKCAjw49vEBhAVEiwADnMbbIWS5ZQBGITtYkRTPYHbHCgNuEkDDPjZVN5UmcfiaIscOBJl_ZdIHRoCN7QQAvD_BwE)

![](https://miro.medium.com/v2/resize:fit:1400/1*vLA9nGVOBYBePYmSyhV_9Q.png)

## Databricks SQL Warehouse

Databricks SQL provides a middle ground with serverless compute but persistent cluster availability.

**Technical Specifications:**

- SQL Compute: $0.22–0.55 per DBU (varies by region)
- Classic SQL Endpoint: 2XS (2 DBU/hr) to 4XL (512 DBU/hr)
- Serverless SQL: Automatic scaling, $0.70 per DBU-hour
- Concurrency: Depends on cluster size

**Real Cost Calculation:** For Medium cluster (16 DBU/hour) in East US 2:

- Hourly cost: 16 × $0.22 = $3.52
- Daily cost (8 hours): $28.16
- Monthly cost: $844.80
- Annual cost: $10,137.60

## Detailed Cost Comparison

![](https://miro.medium.com/v2/resize:fit:1400/1*pCBV9NSAdOiE4L-OAXfHxA.png)

## Cost Analysis Verdict

For my Power BI Import mode scenario with 400GB daily processing:

***Approx.***

- Serverless SQL Pool: $720/year
- Dedicated SQL Pool: $130,637/year (181x more expensive)
- Databricks SQL: $10,138/year (14x more expensive)

Synapse Serverless SQL Pool provides a 99.4% cost reduction compared to Dedicated SQL Pool for this workload pattern.

## Implementation: Building the Integration

Now let me walk through the actual implementation, focusing on the key components that make this architecture work.

## Step 1: Star Schema Design in Databricks

In my Databricks Gold layer, I implement a traditional star schema optimized for analytical queries. The key is designing dimension tables that answer business questions while keeping fact tables lean and efficient.

My approach includes:

- Customer dimension with RFM (Recency, Frequency, Monetary) segmentation
- Date dimension with fiscal calendar support
- Product dimension with category hierarchies
- Fact tables partitioned by date for optimal query performance

I use Delta format exclusively for its ACID transactions, time travel capabilities, and optimization features like Z-Ordering and automatic file compaction.

![](https://miro.medium.com/v2/resize:fit:1400/1*bKSLkWnMZHA4xDXvXiOgJA.png)

## Step 2: Creating Views in Synapse Serverless SQL Pool

The critical component is creating external views that read Delta files directly from ADLS Gen2. This approach eliminates data duplication while providing a SQL interface for Power BI.

Each view maps to a Delta table location in the data lake. When Power BI queries these views, Synapse Serverless SQL Pool:

1. Reads Delta transaction logs to understand current table state
2. Identifies relevant Parquet files based on query predicates
3. Pushes down filters and column pruning to minimize data scanned
4. Returns results without maintaining any persistent compute resources

![](https://miro.medium.com/v2/resize:fit:1400/1*cCzOqFglUAGKm5fobMn7xA.png)

## Step 3: Power BI Data Model Configuration

In Power BI, I establish connection to Synapse Serverless SQL Pool using the standard SQL Server connector. The critical decisions at this stage:

***Critical Consideration for DirectQuery:** Serverless SQL Pool shines when you have a compact Power BI model using Import mode with incremental refresh. The key to success is designing your model to be as compact and efficient as possible. However, for large, complex DirectQuery models with heavy user interaction, Serverless SQL Pool may struggle due to the lack of result caching and concurrent query limitations. In these scenarios, the performance degradation can outweigh the cost benefits, making **Dedicated SQL Pool or Databricks SQL Warehouse **better choices despite their higher costs.*

**Import vs DirectQuery Mode:** I chose Import mode because:

- My data freshness requirement is 4 times daily, not real-time
- Import mode provides better end-user query performance
- Cost is predictable and limited to refresh times
- Complex DAX calculations perform better on imported data

**Incremental Refresh Strategy:** For large fact tables, I implement incremental refresh:

- Historical data (older than 1 year): Refreshed monthly
- Recent data (last 12 months): Refreshed daily
- Current month: Refreshed 4 times daily

This strategy reduces daily data scan from potentially terabytes to just gigabytes, directly impacting cost.

![](https://miro.medium.com/v2/resize:fit:1400/1*_Q9bHjbjUkwryLrVjEMFNA.png)

![](https://miro.medium.com/v2/resize:fit:1400/1*Xijom-hTqVJcKDo-6LvGvg.png)

![](https://miro.medium.com/v2/resize:fit:1400/1*wsCPimZtR4XbDoi9g7BOUw.png)

![](https://miro.medium.com/v2/resize:fit:1400/1*MMlf-G3uXwRvOm5ONq3Rug.png)

## Step 4: Performance Optimization Techniques

Through extensive testing, I’ve identified several critical optimizations:

**Delta File Optimization:** Regular OPTIMIZE commands in Databricks compact small files into larger ones, reducing the number of files Synapse needs to scan. This can reduce query time by 50–80%.

**Z-Ordering Strategy:** I apply Z-ORDER on commonly filtered columns (typically date and key dimensions). This co-locates related data, dramatically improving query performance.

**Partition Elimination:** By partitioning fact tables by year/month and ensuring Power BI filters include date ranges, I reduce data scanned by 90% or more.

**Column Pruning:** Views only select required columns. Since Delta uses columnar storage, selecting fewer columns directly reduces data scanned and cost.

## Critical Lessons Learned

After running this architecture in production, here are the non-obvious insights:

## 1. File Count Matters More Than File Size

I discovered that 1000 files of 100MB each perform worse than 100 files of 1GB each, even though the total data is the same. Synapse Serverless SQL Pool has overhead for each file opened.

## 2. Statistics Are Crucial

Delta table statistics significantly impact query performance. Running ANALYZE TABLE commands after major data loads is essential.

## 3. Timeout Configuration Is Critical

Power BI’s default timeout of 10 minutes can be insufficient for large initial loads. Remember that Serverless SQL Pool has a fixed 30-minute timeout that cannot be changed.

## 4. Cost Monitoring Is Essential

Set up Azure Cost Management alerts for Synapse Serverless SQL Pool. Unexpected cost spikes usually indicate inefficient queries that need optimization.

## 5. View Abstraction Provides Flexibility

Using views as an abstraction layer allows you to reorganize underlying Delta tables without breaking Power BI reports. This flexibility is invaluable for long-term maintenance.

## 6. Understanding Concurrency Limits

What many people misunderstand about Serverless SQL Pool is the concurrency model. While Microsoft states it can handle “1,000 active sessions,” the reality is more nuanced: it can only execute 15 requests simultaneously. The rest get queued. This is still plenty for most Power BI Import mode scenarios, but it’s important to understand when planning your architecture.

## What’s Next: Microsoft Fabric SQL Analytics Endpoint

Looking ahead, my next evolution will be migrating from Synapse Serverless SQL Pool to Microsoft Fabric’s SQL Analytics Endpoint. This migration promises:

- Unified SaaS experience with no infrastructure management
- OneLake integration for simplified data access
- Built-in semantic models shareable across the organization
- Potential for further cost optimization through capacity-based pricing

I’ll cover this migration in detail in my next article, including lessons learned and a complete migration guide.

## Key Takeaways

1. **Synapse Serverless SQL Pool offers exceptional value** for Power BI Import mode scenarios, providing enterprise capabilities at a fraction of the cost of traditional solutions.
2. **Architecture decisions should be data-driven**: My analysis showed 99.4% cost savings by choosing Serverless over Dedicated SQL Pool for this specific workload pattern.
3. **Delta Lake format is essential**: The combination of Delta’s optimization features with Serverless SQL Pool’s pay-per-query model creates a powerful and cost-effective solution.
4. **Optimization compounds savings**: Each optimization technique (partitioning, Z-Ordering, file compaction) directly translates to cost reduction in a pay-per-query model.

*Building production-ready data platforms is a journey of continuous learning and optimization. If you found value in this series, I’d love to hear about your own experiences. Have you tackled similar challenges? Found creative solutions to cost optimization? Hit unexpected roadblocks?*

*Drop a comment below sharing your story, or let’s connect on LinkedIn to continue the conversation. [LinkedIn](https://linkedin.com/in/yasarkocyigit).*