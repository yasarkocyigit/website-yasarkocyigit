---
title: "Building a Production-Ready Data Pipeline with Azure (Part 8): Enterprise Row-Level Security"
date: 2025-08-13
index: 11
summary: "Enforce enterprise row-level security across Power BI, Unity Catalog, and Fabric with dynamic territories and auditing."
tags:
  - Row-Level Security
  - Power BI
  - Unity Catalog
  - Security
draft: false
---
## Building a ProductionReady Data Pipeline with Azure Part 8: Implementing Enterprise Row-Level Security in Power BI

![](https://miro.medium.com/v2/resize:fit:1400/0*KdyE7u25xljWg7UB.png)

*Securing your analytics layer with dynamic, territory-based access control*

Welcome to Part 8 of my comprehensive series on building production-ready data pipelines with Azure. After establishing our complete data platform and Power BI integration, today I’m addressing a critical requirement for enterprise analytics: **Row-Level Security **in Power BI.

## The Journey So Far

Let me quickly recap our architectural evolution:

[**Part 1: Complete Guide to Medallion Architecture](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-complete-guide-to-medallion-architecture-09d73f0e62dd)**
I established the foundation with Azure Data Factory, ADLS Gen2, and the medallion architecture pattern.

[**Part 2: Unity Catalog Integration](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-2-unity-catalog-integration-689b52cb71b2)**
I enhanced the platform with Unity Catalog for unified governance and data lineage tracking.

[**Part 3: Advanced Unity Catalog Table Management](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-3-advanced-unity-catalog-table-d456edbd7f47)**
I explored external tables, managed tables, and data quality constraints.

[**Part 4: From Mount Points to Unity Catalog](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-4-from-mount-points-to-unity-catalog-696f8dd634a1)**
I modernized data access patterns by migrating from mount points to Unity Catalog volumes.

[**Part 5: Implementing CI/CD](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-5-implementing-ci-cd-for-azure-data-fb19c679311b)**
I automated deployments with Azure DevOps pipelines for consistent, reliable releases.

[**Part 6: Gold Layer Implementation**
](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-6-implementing-the-gold-layer-with-c10b141dd7f3)I built sophisticated Gold layer processing with dependency management and flexible scheduling.

[**Part 7: Power BI Integration with Synapse**
](https://medium.com/@kocyigityasar/building-a-production-ready-data-pipeline-with-azure-part-7-power-bi-integration-with-synapse-2346fdaf6d7f)I achieved 97.6% cost reduction by integrating Power BI through Synapse Serverless SQL Pool.

Today, I’m completing our enterprise analytics platform by implementing **Row-Level Security** - ensuring users only see the data they’re authorized to access.

## The Business Challenge

In any enterprise analytics deployment, data access control is paramount. Consider these common scenarios:

- **Territory Managers** should only see sales data for their assigned regions
- **Regional Directors** need visibility across multiple territories in their region
- **C-Suite Executives** require global access to all data
- **External Partners** must be restricted to specific customer segments
- **Compliance Teams** need filtered views based on regulatory requirements

Without proper security implementation, organizations typically resort to:

- Creating dozens of duplicate reports with manual filters
- Maintaining complex folder structures with different permissions
- Building custom applications for data access
- Implementing database-level security that doesn’t scale

These approaches lead to maintenance nightmares, inconsistent metrics, and security vulnerabilities.

## The Solution: Dynamic Row-Level Security at Scale

I’ll implement a comprehensive RLS solution that provides:

- **Dynamic filtering** based on user identity
- **Single source of truth** with one report serving all users
- **Seamless Azure AD integration** for authentication
- **Scalability** to thousands of users without performance degradation
- **Maintainability** through centralized security management

## Architecture Overview

![](https://miro.medium.com/v2/resize:fit:1400/1*5uD55ICb5CiRHIERea2k0Q.png)

## Step 1: Creating the Security Framework in Databricks

First, I’ll establish the security tables in our Gold layer. The key is creating a flexible structure that can handle various access patterns:

```python
# Databricks notebook source
# MAGIC %md
# MAGIC ## RLS Table Creation for Power BI Security
# MAGIC 
# MAGIC This notebook creates security tables for territory-based access control.
# MAGIC In production, this data would typically be maintained through:
# MAGIC - HR systems integration
# MAGIC - Active Directory group membership
# MAGIC - User-friendly web interfaces
# MAGIC - Excel/SharePoint lists for business users
# MAGIC 
# MAGIC For this implementation, I'm using direct table creation for clarity.



# COMMAND ----------
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime
spark = SparkSession.builder.appName("RLS_Setup").getOrCreate()
# Configuration
config = {
    "catalog": "main",
    "schema": "gold_prod",
    "delta_path": "abfss://datalake@storage.dfs.core.windows.net/gold/security/"
}
# COMMAND ----------
# MAGIC %md
# MAGIC ### Understanding the Security Model
# MAGIC 
# MAGIC The security model uses a simple but powerful approach:
# MAGIC - Each user-territory combination is a row
# MAGIC - Special flag (-1) indicates access to all territories
# MAGIC - Multiple rows per user enable complex access patterns
# COMMAND ----------
# Define the security table schema
user_security_schema = StructType([
    StructField("security_id", IntegerType(), False),
    StructField("user_email", StringType(), False),
    StructField("user_name", StringType(), True),
    StructField("territory_key", IntegerType(), False),
    StructField("access_level", StringType(), True),
    StructField("is_active", BooleanType(), False),
    StructField("valid_from", StringType(), False),
    StructField("valid_to", StringType(), True),
    StructField("created_date", TimestampType(), False),
    StructField("created_by", StringType(), False)
])
# Sample security configuration
# In production, this would come from an external source
user_security_data = [
    # Territory Managers - Single territory access
    (1, "territory.manager1@company.com", "Territory Manager 1", 1, "View", 
     True, "2024-01-01", None, datetime.now(), "system"),
    
    # Regional Managers - Multiple territory access
    (2, "regional.manager@company.com", "Regional Manager", 1, "Edit", 
     True, "2024-01-01", None, datetime.now(), "system"),
    (3, "regional.manager@company.com", "Regional Manager", 2, "Edit", 
     True, "2024-01-01", None, datetime.now(), "system"),
    (4, "regional.manager@company.com", "Regional Manager", 3, "Edit", 
     True, "2024-01-01", None, datetime.now(), "system"),
    
    # Executives - All territories (using -1 flag)
    (5, "executive@company.com", "Executive User", -1, "Admin", 
     True, "2024-01-01", None, datetime.now(), "system"),
]
# Create and process the DataFrame
user_security_df = spark.createDataFrame(user_security_data, schema=user_security_schema)
user_security_df = user_security_df.withColumn("valid_from", to_date(col("valid_from")))
# COMMAND ----------
# MAGIC %md
# MAGIC ### Expanding for Power BI Consumption
# MAGIC 
# MAGIC Power BI needs a denormalized view where:
# MAGIC - The -1 flag is expanded to actual territory keys
# MAGIC - Only active users are included
# MAGIC - The structure is optimized for filter performance
# COMMAND ----------
# Define all available territories
# This should match your dim_territory table
all_territories = spark.range(1, 11).select(col("id").alias("territory_key")).collect()
territory_list = [row.territory_key for row in all_territories]
# Separate specific and all-access users
specific_territory_users = user_security_df.filter(
    (col("territory_key") != -1) & 
    (col("is_active") == True)
)
all_territory_users = user_security_df.filter(
    (col("territory_key") == -1) & 
    (col("is_active") == True)
)
# Expand -1 to all territories
expanded_frames = []
for territory in territory_list:
    expanded = all_territory_users.withColumn("territory_key", lit(territory))
    expanded_frames.append(expanded)
# Combine all access patterns
if expanded_frames:
    all_expanded = expanded_frames[0]
    for df in expanded_frames[1:]:
        all_expanded = all_expanded.union(df)
    final_access_df = specific_territory_users.union(all_expanded)
else:
    final_access_df = specific_territory_users
# Create the final access table
user_territory_access = final_access_df.select(
    "user_email",
    "user_name",
    "territory_key",
    "access_level",
    "is_active"
).distinct()
# Write to Delta Lake
output_path = f"{config['delta_path']}user_territory_access"
user_territory_access.write \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .format("delta") \
    .save(output_path)
print(f"✅ Security table written to: {output_path}")
```

![](https://miro.medium.com/v2/resize:fit:1400/1*If0gGLLCUrvYLlxk0TP1yQ.png)

**Using Synapse Serverless SQL Pool Engine for Power Bi Import Model :**

![](https://miro.medium.com/v2/resize:fit:1400/1*wEh8CUk3okqjYrqdVfb-5g.png)

## Step 2: Alternative Maintenance Approaches

While I’m using Databricks for this demonstration, in production environments, consider these more user-friendly approaches:

## Option 1: Excel/CSV Upload

```lua
# Read security configuration from uploaded Excel file
security_excel = spark.read \
    .format("com.crealytics.spark.excel") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/mnt/config/user_territory_mapping.xlsx")
```

## Option 2: SharePoint List Integration

```graphql
# Connect to SharePoint list using Microsoft Graph API
# Sync security configurations daily
# Provides business-user-friendly interface for updates
```

## Option 3: Web Application

```bash
# Build a simple Flask/FastAPI application
# Provide UI for security administrators
# Include approval workflows and audit trails
```

## Option 4: Active Directory Integration

```shell
# Sync with AD security groups
# Map AD groups to territory access
# Automatic updates when users change roles
```

## Step 3: Creating Synapse Views for Power BI

Now I’ll expose the security table through Synapse Serverless SQL Pool:

```sql
-- Create the database and schema if not exists
IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = 'GoldLayer')
    CREATE DATABASE GoldLayer;
GO

USE GoldLayer;
GO
IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'star_schema')
    EXEC('CREATE SCHEMA star_schema');
GO
-- Create the user territory access view
CREATE OR ALTER VIEW star_schema.user_territory_access AS
SELECT 
    CAST(user_email AS VARCHAR(255)) as user_email,
    CAST(user_name AS VARCHAR(255)) as user_name,
    CAST(territory_key AS INT) as territory_key,
    CAST(access_level AS VARCHAR(50)) as access_level
FROM OPENROWSET(
    BULK 'abfss://datalake@storage.dfs.core.windows.net/gold/security/user_territory_access/',
    FORMAT = 'DELTA'
) AS [r]
WHERE is_active = 1;
GO
-- Verify the view
SELECT 
    user_email,
    COUNT(DISTINCT territory_key) as territory_count,
    STRING_AGG(CAST(territory_key AS VARCHAR), ',') as territories
FROM star_schema.user_territory_access
GROUP BY user_email
ORDER BY user_email;
```

## Step 4: The Critical Power BI Configuration

This is where most RLS implementations fail. The relationship configuration must be precise:

## Import the Security Table

1. **Get Data** → Azure Synapse Analytics
2. Server: `your-synapse.sql.azuresynapse.net`
3. Database: `GoldLayer`
4. Data Connectivity mode: Import
5. Select: `star_schema.user_territory_access`

## Configure the Relationship (Most Critical Step!)

1. Go to **Model View**
2. Create relationship:

- From: `user_territory_access[territory_key]`
- To: `dim_territory[territory_key]`
- Cardinality: **Many-to-One (*:1)**
- Cross-filter direction: **Single**
- **Make this relationship active: Yes**
- **Apply security filter in both directions: Yes** ← **CRITICAL!**

The “Apply security filter in both directions” setting is what makes RLS work properly. Without it, the security filter won’t propagate through to your fact tables.

## Create the RLS Role

1. **Modeling** tab → **Manage Roles**
2. Create new role: “Territory Security”
3. Select `user_territory_access` table
4. Add DAX filter:

```csharp
[user_email] = USERPRINCIPALNAME()
```

![](https://miro.medium.com/v2/resize:fit:1400/1*p7bSkad1VhY-OlxsAG-2Bw.png)

1. Save the role

## Test in Power BI Desktop

1. **Modeling** → **View as**
2. Select “Territory Security” role
3. Other user: `territory.manager1@company.com`
4. Verify only Territory 1 data is visible

![](https://miro.medium.com/v2/resize:fit:1400/1*InwcLTsRs9b8vTCcp9LfLg.png)

![](https://miro.medium.com/v2/resize:fit:1400/1*SATmYXDx212DoMNfmk-PTg.png)

**BEFORE RLS FILTERED**

![](https://miro.medium.com/v2/resize:fit:1400/1*RHnfs0UtUTLV-3zCIMxS7A.png)

**AFTER RLS FILTERED**

![](https://miro.medium.com/v2/resize:fit:1400/1*oj8xT4tFj29WscXx7GxZBg.png)

## Step 5: Understanding the Filter Flow

The RLS filter flows through the model like this:

```sql
1. User logs in → email@company.com
2. USERPRINCIPALNAME() returns their email
3. Filter applied: user_territory_access[user_email] = "email@company.com"
4. This filters to specific territory_key values
5. Relationship to dim_territory filters the dimension
6. dim_territory relationship to fact_sales filters the facts
7. User sees only their authorized data
```

## Step 6: Deployment to Power BI Service

## Publish and Configure

1. Publish report to Power BI Service
2. Navigate to dataset settings
3. Go to **Security** tab
4. On “Territory Security” role, add members:

- Add individual users by email
- Or add Azure AD security groups

## Testing in Service

1. Share the report with test users
2. Each user logs in with their credentials
3. Verify they see only their authorized territories
4. Check performance metrics for any degradation

## Performance Optimization

For large-scale deployments, consider these optimizations:

## 1. Aggregation Tables

```sql
-- Create pre-aggregated tables by territory
CREATE TABLE gold.sales_by_territory_daily AS
SELECT 
    territory_key,
    date_key,
    SUM(sales_amount) as total_sales,
    COUNT(DISTINCT customer_key) as unique_customers
FROM fact_sales
GROUP BY territory_key, date_key;
```

## 2. Composite Models

Use DirectQuery for security tables and Import for fact data:

- Security changes reflect immediately
- Fact data remains performant

## 3. Incremental Refresh with RLS

Configure incremental refresh that respects RLS filters to optimize data loading.

## Monitoring and Auditing

Implement comprehensive monitoring:

```ini
# Create audit table for RLS access
audit_schema = StructType([
    StructField("audit_id", StringType(), False),
    StructField("user_email", StringType(), False),
    StructField("access_timestamp", TimestampType(), False),
    StructField("territories_accessed", ArrayType(IntegerType()), True),
    StructField("report_name", StringType(), True),
    StructField("access_method", StringType(), True)  # Desktop/Service/Mobile
])
```

## Pitfall 1: RLS Not Working

**Solution:** Check “Apply security filter in both directions” setting

## Pitfall 2: Users See No Data

**Solution:** Verify email addresses match exactly (case-sensitive)

## Pitfall 3: Performance Degradation

**Solution:** Implement aggregation tables and optimize relationships

## Pitfall 4: Maintenance Overhead

**Solution:** Automate security table updates from source systems

## Pitfall 5: Testing Complexity

**Solution:** Create test users for each access pattern

## Best Practices

1. **Start Simple:** Begin with basic territory filtering, then add complexity
2. **Document Everything:** Maintain clear documentation of who has access to what
3. **Regular Audits:** Schedule monthly access reviews
4. **Performance Testing:** Test with maximum expected concurrent users
5. **Backup Plans:** Have manual override procedures for emergencies
6. **Change Management:** Implement approval workflows for access changes
7. **Training:** Ensure report developers understand RLS implications

## Conclusion

Row-Level Security transforms Power BI from a reporting tool into an enterprise analytics platform. By implementing dynamic, maintainable security at the data model level, I’ve ensured that:

- **Each user sees only their authorized data** without creating multiple reports
- **Security is centrally managed** and easily auditable
- **Performance remains optimal** even with thousands of users
- **Maintenance is simplified** through automated processes

The key to success lies in:

1. Proper relationship configuration (especially the “Apply security filter in both directions” setting)
2. Thoughtful security table design that can handle various access patterns
3. User-friendly maintenance processes that don’t require technical expertise
4. Comprehensive testing before production deployment

*Have questions about RLS implementation or want to share your security challenges? Leave a comment below or connect with me on [LinkedIn](https://linkedin.com/in/yasarkocyigit).*

#PowerBI #RowLevelSecurity #Azure #DataSecurity #Analytics #Synapse #Databricks #EnterpriseAnalytics #DataGovernance #BusinessIntelligence